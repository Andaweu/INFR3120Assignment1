<!DOCTYPE html>

<html lang="en">
    <head>
        <meta charset="UTF-8">
        
        <link type="text/css" rel="stylesheet" href="assignment1style.css" media="only screen and (min-width:960px)">
        <link type="text/css" rel="stylesheet" href="tabletstyle.css" media="only screen and (min-width:481px) and (max-width:959px)">
        <link type="text/css" rel="stylesheet" href="mobilestyle.css" media="only screen and (max-width:480px)">
        <!--Ubuntu font from fonts.google.com-->
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Ubuntu:ital,wght@0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel="stylesheet">
        <title>Andrew Chester's Portfolio</title>
    </head>

    <body>
        <div id="wrapper">
            <header id="pageHeader">
                <div class="headerContainer">
                    <a><img src="media/Face_Picture.jpg" id="headerImage" width="40" height="40" alt="A picture of Andrew Chester"></a>
                    <div class="navBar">
                        
                        <a href="index.html">Home</a>
                        <a href="AaboutMe.html">About Me</a>
                        <a href="Aprojects.html">Projects</a>
                        <a href="AcontactMe.html">Contact Me</a>
                    </div>
                </div>
            </header>

        <div id="mainContent">
            <!--Code was formatted using text-html.com-->
            <article class="project">
                <h1>Project 1: Breadth-First Search(BFS) and Depth-First Search (DFS) deployment in Python</h1>
                <p>Runs the BFS and DFS algorithms using the stack and queue methods. BFS searches by checking each edge node then searching each of those nodes in a stack. DFS searches by checking the a nodes edges then searching the first edge for its edges until it has reached the bottom.</p>
                <div class ="pythonCode">
                <code>
                    

                    <br>from my_queue import Queue
                    <br>from my_stack import Stack
                    <br>import networkx as nx
                    <br>import matplotlib.pyplot as plt
                    <br>
                    <br>def BFS(graph, root):
                    <br>&nbsp; &nbsp; """
                    <br>&nbsp; &nbsp; Performs a Breadth-First Search on a given graph
                    <br>&nbsp; &nbsp; to output a spanning subgraph of the original graph,
                    <br>&nbsp; &nbsp; and the sequence of nodes are visited.
                    <br>&nbsp; &nbsp; """
                    <br>&nbsp; &nbsp; #Your code here
                    <br><br>
                    <br>&nbsp; &nbsp; visitedNodes = set() #creates a set for visited nodes
                    <br>&nbsp; &nbsp; queue = Queue() #creates a queue from the class my_queue.py
                    <br>&nbsp; &nbsp; sequence = [] #list to store the sequence of the nodes
                    <br>&nbsp; &nbsp; spanningTree = nx.Graph()
                    <br>
                    <br>&nbsp; &nbsp; queue.push(root) #sets the root node as the first value in queue
                    <br>&nbsp; &nbsp; visitedNodes.add(root) #adds the root node to visited node
                    <br>&nbsp; &nbsp; while not queue.is_empty(): #runs until the queue is empty
                    <br>&nbsp; &nbsp; &nbsp; &nbsp; pointer = queue.pop() #pops the first item in the queue and points to it
                    <br>&nbsp; &nbsp; &nbsp; &nbsp; sequence.append(pointer) #adds the pointed item to the sequence of visited nodes
                    <br>&nbsp; &nbsp; &nbsp; &nbsp;
                    <br>&nbsp; &nbsp; &nbsp; &nbsp; for i in graph.neighbors(pointer):
                    <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if i not in visitedNodes: #for a node not in previously visited node
                    <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; visitedNodes.add(i) #mark the node as visited
                    <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; queue.push(i) #pushes the current node to the queue
                    <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; spanningTree.add_edge(pointer, i) #adds an edge to the graph for the pointers location
                    <br>&nbsp; &nbsp; return spanningTree, sequence #returns the graph and the BFS sequence
                    <br><br>
                    <br>&nbsp; &nbsp;
                    <br><br>
                    <br>def DFS(graph, root):
                    <br>&nbsp; &nbsp; """
                    <br>&nbsp; &nbsp; Performs a Depth-First Search on a given graph starting
                    <br>&nbsp; &nbsp; from root node to output a spanning subgraph of the
                    <br>&nbsp; &nbsp; original graph, the sequence of visited nodes.
                    <br>&nbsp; &nbsp; """
                    <br>&nbsp; &nbsp; #Your code here
                    <br>
                    <br>&nbsp; &nbsp; visitedNode = set() #creates a set for visited nodes
                    <br>&nbsp; &nbsp; stack = Stack() #creates a stack from the class my_stack.py
                    <br>&nbsp; &nbsp; sequence = [] #list to store the sequence of the nodes
                    <br>&nbsp; &nbsp; spanningTree = nx.Graph()
                    <br>
                    <br>&nbsp; &nbsp; stack.push(root) #sets the root as the first value in the stack
                    <br>&nbsp; &nbsp; visitedNode.add(root) #marks the root as the first node visited
                    <br>&nbsp; &nbsp; while not stack.is_empty(): #runs while the stack is not empty
                    <br>&nbsp; &nbsp; &nbsp; &nbsp; pointer = stack.pop() #pops the last item in the stack and points to it
                    <br>&nbsp; &nbsp; &nbsp; &nbsp; sequence.append(pointer) #appends the pointer to the sequence of nodes
                    <br>
                    <br>&nbsp; &nbsp; &nbsp; &nbsp; for i in reversed(list(graph.neighbors(pointer))): #because a stack is used (LIFO) the list is reversed
                    <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if i not in visitedNode: #if the node has not been visited yet
                    <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; visitedNode.add(i) #marks the current node as visited
                    <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; stack.push(i) #pushes the current node to the stack
                    <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; spanningTree.add_edge(pointer, i) #adds an edge to the graph for the current location
                    <br>&nbsp; &nbsp; return spanningTree, sequence #returns the graph and the DFS sequence
                    <br>
                    <br>def show_graph(graph):
                    <br>&nbsp; &nbsp; plt.subplot(111)
                    <br>&nbsp; &nbsp; pos = nx.spring_layout(graph, seed=11)
                    <br>&nbsp; &nbsp; nx.draw(graph, with_labels=True, font_weight='bold', pos=pos)
                    <br>&nbsp; &nbsp; plt.show()
                    <br>
                    <br>def create_tree():
                    <br>&nbsp; &nbsp; g = nx.Graph()
                    <br>&nbsp; &nbsp; for i in range(7):
                    <br>&nbsp; &nbsp; &nbsp; &nbsp; source = i if i != 3 else 0
                    <br>&nbsp; &nbsp; &nbsp; &nbsp; g.add_edge(source, i+1)
                    <br>&nbsp; &nbsp; return g
                    <br><br>
                    <br>graph = nx.erdos_renyi_graph(12, 0.25, seed=42)
                    <br>
                    <br>show_graph(graph)
                    <br>BFSspan, BFSseq = BFS(graph, 0)
                    <br>DFSspan, DFSseq = DFS(graph, 0)
                    <br>
                    <br>print ("BFS Seguence: ", BFSseq)
                    <br>print ("DFS Seguence: ", DFSseq)
                    <br>show_graph(graph)
                    <br>
                    <br>graph = &nbsp;create_tree()
                    <br>show_graph(graph)
                    <br>BFSspan, BFSseq = BFS(graph, 0)
                    <br>DFSspan, DFSseq = DFS(graph, 0)
                    <br>
                    <br>print ("BFS Seguence: ", BFSseq)
                    <br>print ("DFS Seguence: ", DFSseq)
                    <br>>show_graph(graph)
                    <br><br><br>
                    </code>
                    </div>

            </article>
            <br>
            <article class="project">
                <h1>Project 2: Link Tree</h1>
                <p>Takes in a url then scans the page for all other urls it contains, then creates a visual graph to display the findings. This code uses the BFS algorithm to find all the edge nodes from the desired page.</p>
                <div class="pythonCode">
                    <code>
                        <br>from bs4 import BeautifulSoup
                        <br>import urllib3 as urllib
                        <br>import networkx as nx
                        <br>import matplotlib.pyplot as plt
                        <br>
                        <br>def findUrls(root):
                        <br>&nbsp; &nbsp; #Your code here
                        <br>
                        <br>&nbsp; &nbsp; http = urllib.PoolManager() #creates a PoolManager to handle requests
                        <br>&nbsp; &nbsp; graph = nx.Graph() #creates an nx graph
                        <br>&nbsp; &nbsp; graph.add_node(root) #uses the root (base url) as the root node of the star
                        <br>
                        <br>&nbsp; &nbsp; try: #attempt to connect to the root url
                        <br>&nbsp; &nbsp; &nbsp; &nbsp; resp = http.request("GET", root) #sent a GET request to the root url
                        <br>&nbsp; &nbsp; &nbsp; &nbsp; soup = BeautifulSoup(resp.data, 'html.parser') #BeautifulSoup paraphrases the HTML content to search the url Tree
                        <br>
                        <br>&nbsp; &nbsp; &nbsp; &nbsp; for link in soup.find_all('a', href=True): #Loops through all the HTML &lt;a&gt; tags with an href attribute
                        <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; url = link['href'] #takes the href attribute from the url
                        <br>
                        <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if url.startswith('http'): #only looks for urls that start with http
                        <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; graph.add_node(url) #adds the url to the graph
                        <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; graph.add_edge(root, url) #adds an edge between the root and the found url nodes
                        <br>
                        <br>&nbsp; &nbsp;
                        <br>&nbsp; &nbsp; except Exception as error: #creates an exception should the url not connect
                        <br>&nbsp; &nbsp; &nbsp; &nbsp; print(f"error fetching {root}: {error}")
                        <br><br><br>
                        <br>&nbsp; &nbsp; return graph
                        <br><br>
                        <br>"""
                        <br>CODE TO TEST YOUR FUNCTIONS BEGINS HERE (DON'T MODIFY)
                        <br>"""
                        <br>
                        <br>url = "http://google.ca"
                        <br><br>
                        <br>plt.figure(figsize=(12, 10))
                        <br><br>
                        <br>graph = findUrls(url)
                        <br>plt.subplot(111)
                        <br>nx.draw(graph, with_labels=True)
                        <br>plt.show()
                        <br>print('\n Graph Nodes = ', graph.nodes)
                        <br>print('\n Number of URLs = ', len(graph.nodes))
                        
                    </code>
                </div>
            </article>
            <br>
            <article class="project">
                <h1>Project 3: Webcrawler</h1>
                <p>Takes a url and searches the page for other urls, then uses a BFS search to continue scanning each website as far as desired. Creates a save point every 500 pages and creates a graph at the end to visualize the findings.</p>
                <div class="pythonCode">
                    <code>
                            <br>import requests
                            <br>from bs4 import BeautifulSoup
                            <br>import networkx as nx
                            <br>from urllib.parse import urljoin
                            <br>from collections import deque
                            <br>import time
                            <br>import matplotlib.pyplot as plt
                            <br>def is_valid_link(link):
                            <br>&nbsp; &nbsp; if not link or 'http' not in link:
                            <br>&nbsp; &nbsp; &nbsp; &nbsp; return False
                            <br>&nbsp; &nbsp; invalid_suffixes = ['.jpg', '.jpeg', '.png', '.gif', '.pdf', '.svg', '.webp']
                            <br>&nbsp; &nbsp; return not any(link.lower().endswith(ext) for ext in invalid_suffixes)
                            <br>def create_tree():
                            <br>&nbsp; &nbsp; return nx.DiGraph() # or nx.Graph() if you want undirected
                            <br>def crawl(start_url, max_pages=100, save_interval=500, output_file='web_graph.gexf', link_limit=50):
                            <br>&nbsp; &nbsp; visited = set()
                            <br>&nbsp; &nbsp; graph = create_tree() # Use your graph creation function
                            <br>&nbsp; &nbsp; queue = deque([start_url])
                            <br>&nbsp; &nbsp; page_count = 0
                            <br>&nbsp; &nbsp; while queue and page_count &lt; max_pages:
                            <br>&nbsp; &nbsp; &nbsp; &nbsp; current_url = queue.popleft()
                            <br>&nbsp; &nbsp; &nbsp; &nbsp; if current_url in visited:
                            <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; continue
                            <br>&nbsp; &nbsp; &nbsp; &nbsp; print(f"Crawling: {current_url}")
                            <br>&nbsp; &nbsp; &nbsp; &nbsp; visited.add(current_url)
                            <br>&nbsp; &nbsp; &nbsp; &nbsp; try:
                            <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; response = requests.get(current_url, timeout=5)
                            <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if 'text/html' not in response.headers.get('Content-Type', ''):
                            <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; continue
                            <br>&nbsp; &nbsp; &nbsp; &nbsp; except requests.RequestException:
                            <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; continue
                            <br>&nbsp; &nbsp; &nbsp; &nbsp; soup = BeautifulSoup(response.text, 'html.parser')
                            <br>&nbsp; &nbsp; &nbsp; &nbsp; link_counter = 0
                            <br>&nbsp; &nbsp; &nbsp; &nbsp; for a_tag in soup.find_all('a', href=True):
                            <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if link_counter &gt;= link_limit:
                            <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; break
                            <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; href = a_tag['href']
                            <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; full_url = urljoin(current_url, href)
                            <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if is_valid_link(full_url):
                            <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; graph.add_edge(current_url, full_url)
                            <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if full_url not in visited:
                            <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; queue.append(full_url)
                            <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; link_counter += 1
                            <br>&nbsp; &nbsp; &nbsp; &nbsp; page_count += 1
                            <br>&nbsp; &nbsp; &nbsp; &nbsp; if page_count % save_interval == 0:
                            <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; filename = f"{output_file.replace('.gexf', '')}_{page_count}.gexf"
                            <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; nx.write_gexf(graph, filename)
                            <br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print(f"Saved graph with {page_count} pages to {filename}")
                            <br>&nbsp; &nbsp; nx.write_gexf(graph, output_file)
                            <br>&nbsp; &nbsp; print(f"Final graph saved to {output_file} with {page_count} pages.")
                            <br><br><br>
                            <br>def show_graph(graph):
                            <br>&nbsp; &nbsp; plt.figure(figsize=(12, 8)) # Optional: Set figure size
                            <br>&nbsp; &nbsp; pos = nx.spring_layout(graph, seed=11) # Layout for consistent display
                            <br>&nbsp; &nbsp; nx.draw(graph, with_labels=True, font_weight='bold', pos=pos, node_size=500,node_color='skyblue', edge_color='gray')
                            <br>plt.title("Web Graph Visualization")
                            <br>plt.show()
                            <br># Example usage
                            <br>start_url = 'https://google.com'
                            <br>crawl(start_url)
                            <br>if __name__ == "__main__":
                            <br>&nbsp; &nbsp; g = create_tree()
                            <br>&nbsp; &nbsp; show_graph(g)
                            <br>&nbsp; &nbsp; nx.readwrite.gexf.read_gexf('web_graph.gexf')
                            
                        </code> 
                </div>
                
            </article>
            <br>
            <article class="project">
                <h1>Project 4: Home Servers</h1>
                <a><img src="media/server1.jpg" alt="A picture of Andrew's first server"></a>
                <a><img src="media/server2.jpg" alt="A picture of Andrew's second server."></a>
                <p>The first server uses an i7-9700k, 32gb of DDR4 ram, 3x2TB hard drives and 4x500gb hard drives. The second server uses a Ryzen 5 3600, 32gb of DDR4 ram and 4x4TB hard drives
                    Both servers run TrueNas which is a debian based OS. They run an SMB share for storage, a Tailscale VPN and portainer to run docker containers, primarily game servers.
                </p>
            </article>
            <br><br><br>
        </div>
        </div>

        <div class="pageFooter">
            <footer>
                <p>This webpage was created by Andrew Chester <br>©2025 <a href="AcontactMe.html">contact me</a> at andrew.chester@ontariotechu.net</p>
            </footer>
        </div>
    </body>
</html>